{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF2.0_CycleGAN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDz5kRwnuIHSg8yLvn4mcz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JHyunjun/TF2.0_Generative-Adversarial-Network/blob/main/TF2_0_CycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-erid6-JwU-Y"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "FILE=apple2orange\n",
        "rm -rf ./datasets\n",
        "mkdir ./datasets\n",
        "\n",
        "URL=https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/$FILE.zip\n",
        "ZIP_FILE=./datasets/$FILE.zip\n",
        "TARGET_DIR=./datasets/$FILE/\n",
        "wget -N $URL -O $ZIP_FILE\n",
        "mkdir $TARGET_DIR\n",
        "unzip $ZIP_FILE -d ./datasets/\n",
        "rm $ZIP_FILE\n",
        "\n",
        "mkdir -p ./images/$FILE/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "import imageio\n",
        "from skimage.transform import resize\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "\n",
        "class DataLoader():\n",
        "    def __init__(self, dataset_name, img_res=(128, 128)):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.img_res = img_res\n",
        "\n",
        "    def load_data(self, domain, batch_size=1, is_testing=False):\n",
        "        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n",
        "        path = glob('./datasets/%s/%s/*' % (self.dataset_name, data_type))\n",
        "\n",
        "        batch_images = np.random.choice(path, size=batch_size)\n",
        "\n",
        "        imgs = []\n",
        "        for img_path in batch_images:\n",
        "            img = self.imread(img_path)\n",
        "            if not is_testing:\n",
        "                img = resize(img, self.img_res)\n",
        "\n",
        "                if np.random.random() > 0.5:\n",
        "                    img = np.fliplr(img)\n",
        "            else:\n",
        "                img = resize(img, self.img_res)\n",
        "            imgs.append(img)\n",
        "\n",
        "        imgs = np.array(imgs)/127.5 - 1.\n",
        "\n",
        "        return imgs\n",
        "\n",
        "    def load_batch(self, batch_size=1, is_testing=False):\n",
        "        data_type = \"train\" if not is_testing else \"val\"\n",
        "        path_A = glob('./datasets/%s/%sA/*' % (self.dataset_name, data_type))\n",
        "        path_B = glob('./datasets/%s/%sB/*' % (self.dataset_name, data_type))\n",
        "\n",
        "        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n",
        "        total_samples = self.n_batches * batch_size\n",
        "\n",
        "        # Sample n_batches * batch_size from each path list so that model sees all\n",
        "        # samples from both domains\n",
        "        path_A = np.random.choice(path_A, total_samples, replace=False)\n",
        "        path_B = np.random.choice(path_B, total_samples, replace=False)\n",
        "\n",
        "        for i in range(self.n_batches-1):\n",
        "            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n",
        "            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n",
        "            imgs_A, imgs_B = [], []\n",
        "            for img_A, img_B in zip(batch_A, batch_B):\n",
        "                img_A = self.imread(img_A)\n",
        "                img_B = self.imread(img_B)\n",
        "\n",
        "                img_A = resize(img_A, self.img_res)\n",
        "                img_B = resize(img_B, self.img_res)\n",
        "\n",
        "                if not is_testing and np.random.random() > 0.5:\n",
        "                        img_A = np.fliplr(img_A)\n",
        "                        img_B = np.fliplr(img_B)\n",
        "\n",
        "                imgs_A.append(img_A)\n",
        "                imgs_B.append(img_B)\n",
        "\n",
        "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
        "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
        "\n",
        "            yield imgs_A, imgs_B\n",
        "\n",
        "    def imread(self, path):\n",
        "        return imageio.imread(path, pilmode='RGB').astype(np.float)"
      ],
      "metadata": {
        "id": "Y4vud9TG17KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "V8vQFnEZGvp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing\n",
        " \n",
        "from __future__ import print_function, division\n",
        "import scipy\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow_addons.layers import InstanceNormalization\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "oc4tJzck17aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CycleGAN Frame\n",
        "\n",
        "class CycleGAN() : \n",
        "  def __init__(self) : \n",
        "    self.img_rows = 128\n",
        "    self.img_cols = 128\n",
        "    self.channels = 3\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "\n",
        "    self.dataset_name = 'apple2orange'\n",
        "    self.data_loader = DataLoader(dataset_name = self.dataset_name, img_res = (self.img_rows, self.img_cols))\n",
        "\n",
        "    patch = int(self.img_rows / 2**4)\n",
        "    self.disc_patch = (patch, patch, 1)\n",
        "\n",
        "    self.gf = 32\n",
        "    self.df = 64\n",
        "\n",
        "    self.lambda_cycle = 10.0 # Cycle-consistent loss : A - B - A후 첫 A와 최종변환한 A의 Loss를 구했을때의 가중치\n",
        "    self.lambda_id = 0.9 * self.lambda_cycle #Generator로 만든 이미지가 Discriminator없이 실제 이미지와 동일성을 구분하고자 하는 변수의 가중치\n",
        "    # 이게 낮으면, 실제와 형태는 유사하지만 색감 혹은 명도/채도 차이가 발생할 수 있음\n",
        "\n",
        "    optimizer = Adam(0.0002,0.5)\n",
        "\n",
        "    #Cycle-GAN whole Network Design\n",
        "\n",
        "    #Designing Discriminator\n",
        "    self.d_A = self.build_discriminator()\n",
        "    self.d_B = self.build_discriminator()\n",
        "    self.d_A.compile(loss = 'mse', optimizer = optimizer, metrics = ['accuracy'])\n",
        "    self.d_B.compile(loss = 'mse', optimizer = optimizer, metrics = ['accuracy'])\n",
        "\n",
        "    #Designing Generator A to B and B to A(Cycle)\n",
        "    self.g_AB = self.build_generator()\n",
        "    self.g_BA = self.build_generator()\n",
        "\n",
        "    img_A = Input(shape = self.img_shape)\n",
        "    img_B = Input(shape = self.img_shape) \n",
        "\n",
        "    fake_B = self.g_AB(img_A) #generating image from A to B\n",
        "    fake_A = self.g_BA(img_B) #generating image from B to A\n",
        "\n",
        "    reconstructed_A = self.g_BA(fake_B) #generate image from fake B to A\n",
        "    reconstructed_B = self.g_BA(fake_A) #generate image from fake A to B\n",
        "\n",
        "    img_A_id = self.g_BA(img_A) #  Cycle Consistent Loss구할때 결국 원본 제대로 되었는지 확인하는것 (id : identification)\n",
        "    img_B_id = self.g_AB(img_B)\n",
        "\n",
        "    self.d_A.trainable = False # 밑으로는 Generator만 훈련\n",
        "    self.d_B.trainable = False\n",
        "\n",
        "    valid_A = self.d_A(fake_A) #fakeA를 Discriminator가 판단했을때의 결과 확률\n",
        "    valid_B = self.d_B(fake_B) \n",
        "\n",
        "    self.combined = Model(inputs = [img_A, img_B], outputs = [valid_A, valid_B, reconstructed_A, reconstructed_B, img_A_id, img_B_id])\n",
        "    #output 각각은 d_A(fake_A), d_B(fake_B), G_BA(fake_B), G_AB(fake_A), G_BA(img_A), G_AB(img_B)에 대응되어 학습\n",
        "\n",
        "    self.combined.compile(loss = ['mse', 'mse', 'mae', 'mae', 'mae', 'mae'], loss_weights = [1, 1, self.lambda_cycle, self.lambda_cycle, self.lambda_id, self.lambda_id], optimizer = optimizer)\n",
        "  \n",
        "    \n"
      ],
      "metadata": {
        "id": "Q66PNmUd2WFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Design\n",
        "# CNN이 Auto Encoder형식으로 쓰이는 것을 U-net구조라 부름\n",
        "# Image Generator는 기본적으로 AutoEncoder이자 U-net구조임 ; Real Original Image - Down Sampling - Latent Vector - Up Sampling - Generated Original Image\n",
        "class CycleGAN(CycleGAN):\n",
        "      @staticmethod\n",
        "      def conv2d(layer_input, filters, f_size=4, normalization=True):\n",
        "        \"\"\"다운샘플링하는 동안 사용되는 층\"\"\"\n",
        "        d = Conv2D(filters, kernel_size=f_size,\n",
        "                   strides=2, padding='same')(layer_input)\n",
        "        d = LeakyReLU(alpha=0.2)(d)\n",
        "        if normalization:\n",
        "            d = InstanceNormalization()(d)\n",
        "        return d\n",
        "      \n",
        "      @staticmethod\n",
        "      def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
        "            \"\"\"업샘플링하는 동안 사용되는 층\"\"\"\n",
        "            u = UpSampling2D(size=2)(layer_input)\n",
        "            u = Conv2D(filters, kernel_size=f_size, strides=1,\n",
        "                       padding='same', activation='relu')(u)\n",
        "            if dropout_rate:\n",
        "                u = Dropout(dropout_rate)(u)\n",
        "            u = InstanceNormalization()(u)\n",
        "            u = Concatenate()([u, skip_input])\n",
        "            return u"
      ],
      "metadata": {
        "id": "oqEb6GhTc1j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleGAN(CycleGAN):\n",
        "    def build_generator(self):\n",
        "        \"\"\"U-Net 생성자\"\"\"\n",
        "        # 이미지 입력\n",
        "        d0 = Input(shape=self.img_shape)\n",
        "\n",
        "        # 다운샘플링\n",
        "        d1 = self.conv2d(d0, self.gf) #gf는 Filter 갯수로, gf = 32로 위에서 선언\n",
        "        d2 = self.conv2d(d1, self.gf * 2)\n",
        "        d3 = self.conv2d(d2, self.gf * 4)\n",
        "        d4 = self.conv2d(d3, self.gf * 8)\n",
        "\n",
        "        # 업샘플링\n",
        "        u1 = self.deconv2d(d4, d3, self.gf * 4)\n",
        "        u2 = self.deconv2d(u1, d2, self.gf * 2)\n",
        "        u3 = self.deconv2d(u2, d1, self.gf)\n",
        "\n",
        "        u4 = UpSampling2D(size=2)(u3)\n",
        "        output_img = Conv2D(self.channels, kernel_size=4,\n",
        "                            strides=1, padding='same', activation='tanh')(u4)\n",
        "\n",
        "        return Model(d0, output_img) # Original Image와 Generated Original Image를 최종 리턴\n",
        "\n"
      ],
      "metadata": {
        "id": "6I2N-QqRHmv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleGAN(CycleGAN):\n",
        "    def build_discriminator(self):\n",
        "      img = Input(shape=self.img_shape)\n",
        "\n",
        "      d1 = self.conv2d(img, self.df, normalization=False)\n",
        "      d2 = self.conv2d(d1, self.df * 2)\n",
        "      d3 = self.conv2d(d2, self.df * 4)\n",
        "      d4 = self.conv2d(d3, self.df * 8)\n",
        "\n",
        "      validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
        "\n",
        "      return Model(img, validity)"
      ],
      "metadata": {
        "id": "-6J1x7wxdCq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleGAN(CycleGAN):\n",
        "      def sample_images(self, epoch, batch_i):\n",
        "        r, c = 2, 3\n",
        "\n",
        "        imgs_A = self.data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n",
        "        imgs_B = self.data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n",
        "        \n",
        "        fake_B = self.g_AB.predict(imgs_A)\n",
        "        fake_A = self.g_BA.predict(imgs_B)\n",
        "\n",
        "        reconstr_A = self.g_BA.predict(fake_B)\n",
        "        reconstr_B = self.g_AB.predict(fake_A)\n",
        "\n",
        "        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
        "\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        titles = ['Original', 'Translated', 'Reconstructed']\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt])\n",
        "                axs[i, j].set_title(titles[j])\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "Wveez1-lRiZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CycleGAN Main Training Section ★★★\n",
        "\n",
        "class CycleGAN(CycleGAN):\n",
        "  def train(self, epochs, batch_size = 1, sample_interval = 50) : \n",
        "    valid = np.ones((batch_size, ) + self.disc_patch)\n",
        "    fake = np.zeros((batch_size, ) + self.disc_patch)\n",
        "\n",
        "    for epoch in range(epochs) : \n",
        "      for batch_i, (imgs_A, imgs_B) in enumerate(\n",
        "          self.data_loader.load_batch(batch_size)) : \n",
        "\n",
        "          fake_B = self.g_AB.predict(imgs_A) # A-B 즉, 1번 Domain을 바꾼 Generated Original Image\n",
        "          fake_A = self.g_BA.predict(imgs_B)\n",
        "\n",
        "          #Discriminator B to A 학습\n",
        "          dA_loss_real = self.d_A.train_on_batch(imgs_A, valid) \n",
        "          dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n",
        "          dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
        "          \n",
        "          #Discriminator A to B 학습\n",
        "          dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n",
        "          dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n",
        "          dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
        "\n",
        "          d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
        "\n",
        "          g_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, valid, imgs_A, imgs_B, imgs_A, imgs_B])\n",
        "          # 8번 셀에있는 CycleGAN Frame에 있는 combined들의 output 확인\n",
        "\n",
        "          if batch_i % sample_interval ==0 : \n",
        "            self.sample_images(epoch,batch_i)"
      ],
      "metadata": {
        "id": "0dbaKPOnSHY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Operating CycleGAN\n",
        "cycle_gan = CycleGAN()\n",
        "cycle_gan.train(epochs = 100, batch_size = 64, sample_interval = 10)"
      ],
      "metadata": {
        "id": "_akA51oTUzp4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}