{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF2.0_SGAN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNciuov5ifrLPrAuYbb53G4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JHyunjun/TF2.0_Generative-Adversarial-Network/blob/main/TF2_0_SGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlgq9PixNQlo"
      },
      "outputs": [],
      "source": [
        "#importing\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "from tensorflow.keras.layers import (Activation, BatchNormalization, Concatenate, Dense, Dropout, Flatten, Input, Lambda, Reshape)\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MNIST image\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "\n",
        "img_shape = (img_rows, img_cols, channels)\n",
        "z_dim = 100\n",
        "\n",
        "num_classes = 10"
      ],
      "metadata": {
        "id": "mA5kmDJWRWho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data set\n",
        "class Datasets : \n",
        "  def __init__(self, num_labeled) : \n",
        "    self. num_labeled = num_labeled\n",
        "\n",
        "    (self.x_train, self.y_train), (self.x_test, self.y_test) = mnist.load_data()\n",
        "\n",
        "    def preprocess_imgs(x) : #Norm\n",
        "      x = (x.astype(np.float32) - 127.5) / 127.5\n",
        "      x = np.expand_dims(x, axis = 3) # Width X Height X Channel로 확장\n",
        "      return x\n",
        "\n",
        "    def preprocess_labels(y) : \n",
        "      return y.reshape(-1,1)\n",
        "\n",
        "    self.x_train = preprocess_imgs(self.x_train)\n",
        "    self.y_train = preprocess_labels(self.y_train)\n",
        "\n",
        "    self.x_test = preprocess_imgs(self.x_test)\n",
        "    self.y_test = preprocess_labels(self.y_test)\n",
        "\n",
        "  def batch_labeled(self, batch_size ) :\n",
        "    idx = np.random.randint(0, self.num_labeled, batch_size)\n",
        "    imgs = self.x_train[idx]\n",
        "    labels = self.y_train[idx]\n",
        "    return imgs, labels\n",
        "\n",
        "  def batch_unlabeled(self, batch_size) : \n",
        "    idx = np.random.randint(self.num_labeled, self.x_train.shape[0], batch_size) \n",
        "    imgs = self.x_train[idx]\n",
        "    return imgs\n",
        "\n",
        "  def training_set(self) : \n",
        "    x_train = self.x_train[range(self.num_labeled)]\n",
        "    y_train = self.y_train[range(self.num_labeled)]\n",
        "    return x_train, y_train\n",
        "\n",
        "  def test_set(self) : \n",
        "    return self.x_test, self.y_test\n",
        "\n"
      ],
      "metadata": {
        "id": "3k3Lz-_vRjzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_labeled = 100\n",
        "dataset = Datasets(num_labeled)"
      ],
      "metadata": {
        "id": "VfwjxW5Xeqcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Designing Generator\n",
        "\n",
        "def build_generator(z_dim) : \n",
        "  model = Sequential()\n",
        "  model.add(Dense(256 * 7 * 7, input_dim = z_dim))\n",
        "  model.add(Reshape((7,7,256)))\n",
        "\n",
        "  model.add(Conv2DTranspose(128, kernel_size = 3, strides = 2, padding = 'same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha = 0.01))\n",
        "\n",
        "  model.add(Conv2DTranspose(64, kernel_size = 3, strides = 1, padding = 'same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha = 0.01))\n",
        "\n",
        "  model.add(Conv2DTranspose(1, kernel_size = 3, strides = 2, padding = 'same'))\n",
        "  model.add(Activation('tanh'))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "UpWlaCiHfhaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Designing Discriminator\n",
        "\n",
        "def build_discriminator_net(img_shape) : \n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size = 3, strides = 2, padding = 'same'))\n",
        "  model.add(LeakyReLU(alpha = 0.01))\n",
        "\n",
        "  model.add(Conv2D(64, kernel_size = 3, strides = 2, padding = 'same'))\n",
        "  model.add(LeakyReLU(alpha = 0.01))\n",
        "\n",
        "  model.add(Conv2D(128, kernel_size = 3, strides = 2, padding = 'same'))\n",
        "  model.add(LeakyReLU(alpha = 0.01))\n",
        "\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(num_classes))\n",
        "  return model"
      ],
      "metadata": {
        "id": "WrCHsMWPgPFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Discriminator of supervised learning\n",
        "\n",
        "def build_discriminator_supervised(discriminator_net) : \n",
        "  model = Sequential()\n",
        "  model.add(discriminator_net)\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "I99UPzZDg5FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Discriminator of semi-supervised learning\n",
        "#input으로 들어온 이미지가 진짜일지 가짜일지를 predict(x) 함수를 통해 구함\n",
        "#이미지가 가짜일 경우 x값이 전체적으로 작음 -> prediction값이 1에 가깝게 나옴, 반대로 이미지가 진짜일 경우 x값이 전체적으로 크고 prediction값이 0에 가깝게 나옴\n",
        "#즉 prediction은 가짜일 확률로 보면됨\n",
        "\n",
        "def build_discriminator_unsupervised(discriminator_net) : \n",
        "  model = Sequential()\n",
        "  model.add(discriminator_net)\n",
        "\n",
        "  def predict(x) : \n",
        "    prediction = 1.0 - (1.0 / (K.sum(K.exp(x), axis = -1, keepdims = True) + 1.0))\n",
        "    return prediction\n",
        "\n",
        "  model.add(Lambda(predict))\n",
        "  return model"
      ],
      "metadata": {
        "id": "vIRXQOJbhOq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build GAN\n",
        "# [1]Supervised-learning기법으로 이미 라벨링된 이미지를 통해 학습시킴\n",
        "# [2]Unsupervised-learning기법으로 [1]로 학습시킨 Discriminator가 진짜이미지와 가짜이미지를 구분하도록 학습\n",
        "# [3]Generator만 학습하기 위해 Discriminator 학습을 끔\n",
        "# [4]z_dim기반으로 Generator가 만든 이미지를 [2]까지 학습시킨 Discriminator를 통해, 참-거짓을 구분하고 Generator 학습 \n",
        "# 단점은 Generator가 학습할때 Reference로 삼는 Disctrimiator의 예측값이 반드시 참이라는 보장이 없음 → Discriminator 오차가 누적된 Geneator 오차 발생 가능\n",
        "\n",
        "def build_gan(generator, discriminator) : \n",
        "  model = Sequential()\n",
        "  model.add(generator)\n",
        "  model.add(discriminator)\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "5UaxQA2MkM_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator_net = build_discriminator_net(img_shape) \n",
        "discriminator_supervised = build_discriminator_supervised(discriminator_net) #[1]\n",
        "discriminator_supervised.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = Adam(learning_rate = 0.0003))\n",
        "\n",
        "discriminator_unsupervised = build_discriminator_unsupervised(discriminator_net) #[2]\n",
        "discriminator_unsupervised.compile(loss = 'binary_crossentropy', optimizer=Adam())\n",
        "\n",
        "generator = build_generator(z_dim)\n",
        "discriminator_unsupervised.trainable = False #[3]\n",
        "\n",
        "gan = build_gan(generator, discriminator_unsupervised) #[4]\n",
        "gan.compile(loss = 'binary_crossentropy', optimizer = Adam())"
      ],
      "metadata": {
        "id": "LJQdxsXP1PZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pipeline\n",
        "supervised_losses = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "\n",
        "def train(iterations, batch_size, sample_interval):\n",
        "\n",
        "    # 진짜 이미지의 레이블: 모두 1\n",
        "    real = np.ones((batch_size, 1))\n",
        "\n",
        "    # 가짜 이미지의 레이블: 모두 0\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        # -------------------------\n",
        "        #  판별자 훈련\n",
        "        # -------------------------\n",
        "\n",
        "        # 레이블된 샘플을 가져옵니다.\n",
        "        imgs, labels = dataset.batch_labeled(batch_size)\n",
        "\n",
        "        # 레이블을 원-핫 인코딩합니다.\n",
        "        labels = to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "        # 레이블이 없는 샘플을 가져옵니다.\n",
        "        imgs_unlabeled = dataset.batch_unlabeled(batch_size)\n",
        "\n",
        "        # 가짜 이미지의 배치를 생성합니다.\n",
        "        z = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "        gen_imgs = generator.predict(z)\n",
        "\n",
        "        # 레이블된 진짜 샘플에서 훈련합니다.\n",
        "        d_loss_supervised, accuracy = discriminator_supervised.train_on_batch(imgs, labels)\n",
        "\n",
        "        # 레이블이 없는 진짜 샘플에서 훈련합니다.\n",
        "        d_loss_real = discriminator_unsupervised.train_on_batch(\n",
        "            imgs_unlabeled, real)\n",
        "\n",
        "        # 가짜 샘플에서 훈련합니다.\n",
        "        d_loss_fake = discriminator_unsupervised.train_on_batch(gen_imgs, fake)\n",
        "\n",
        "        d_loss_unsupervised = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # ---------------------\n",
        "        #  생성자 훈련\n",
        "        # ---------------------\n",
        "\n",
        "        # 가짜 이미지의 배치를 생성합니다.\n",
        "        z = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "        gen_imgs = generator.predict(z)\n",
        "\n",
        "        # 생성자를 훈련합니다.\n",
        "        g_loss = gan.train_on_batch(z, np.ones((batch_size, 1)))\n",
        "\n",
        "        if (iteration + 1) % sample_interval == 0:\n",
        "\n",
        "            # 훈련이 끝난 후 그래프를 그리기 위해 판별자의 지도 학습 분류 손실을 기록합니다.\n",
        "            supervised_losses.append(d_loss_supervised)\n",
        "            iteration_checkpoints.append(iteration + 1)\n",
        "\n",
        "            # 훈련 과정을 출력합니다.\n",
        "            print(\n",
        "                \"%d [D loss supervised: %.4f, acc.: %.2f%%] [D loss unsupervised: %.4f] [G loss: %f]\"\n",
        "                % (iteration + 1, d_loss_supervised, 100 * accuracy,\n",
        "                   d_loss_unsupervised, g_loss))"
      ],
      "metadata": {
        "id": "QRxtXqBXp5sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running\n",
        "iterations = 8000\n",
        "batch_size = 32\n",
        "sample_interval = 100\n",
        "\n",
        "train(iterations, batch_size, sample_interval)"
      ],
      "metadata": {
        "id": "06Z25LGEsx5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = np.array(supervised_losses)\n",
        "\n",
        "# 판별자의 지도 학습 손실을 그립니다.\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(iteration_checkpoints, losses, label=\"Discriminator loss\")\n",
        "\n",
        "plt.xticks(iteration_checkpoints, rotation=90)\n",
        "\n",
        "plt.title(\"Discriminator – Supervised Loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gBDUItIQ80mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = dataset.training_set()\n",
        "y = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "# 훈련 세트에서 분류 정확도 계산\n",
        "_, accuracy = discriminator_supervised.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "metadata": {
        "id": "8pohaOfi831z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = dataset.test_set()\n",
        "y = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "# 테스트 세트에서 분류 정확도 계산\n",
        "_, accuracy = discriminator_supervised.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "metadata": {
        "id": "yhuqQc168519"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SGAN 판별자와 같은 네트워크 구조를 가진 지도 학습 분류기\n",
        "mnist_classifier = build_discriminator_supervised(build_discriminator_net(img_shape))\n",
        "mnist_classifier.compile(loss='categorical_crossentropy',\n",
        "                         metrics=['accuracy'],\n",
        "                         optimizer=Adam())"
      ],
      "metadata": {
        "id": "t0qdBXy687sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, labels = dataset.training_set()\n",
        "\n",
        "# 레이블을 원-핫 인코딩합니다.\n",
        "labels = to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "# 분류기를 훈련합니다.\n",
        "training = mnist_classifier.fit(x=imgs,\n",
        "                                y=labels,\n",
        "                                batch_size=32,\n",
        "                                epochs=30,\n",
        "                                verbose=1)\n",
        "losses = training.history['loss']\n",
        "accuracies = training.history['accuracy']"
      ],
      "metadata": {
        "id": "snoInCZR89xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 분류 손실을 그립니다\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.array(losses), label=\"Loss\")\n",
        "plt.title(\"Classification Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T-GtTEZ-8_wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 분류 정확도를 그립니다.\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.array(accuracies), label=\"Accuracy\")\n",
        "plt.title(\"Classification Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T690dnxz9Bgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = dataset.training_set()\n",
        "y = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "# 훈련 세트에 대한 분류 정확도를 계산합니다.\n",
        "_, accuracy = mnist_classifier.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "metadata": {
        "id": "mdepa50K9C-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = dataset.test_set()\n",
        "y = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "# 테스트 세트에 대한 분류 정확도를 계산합니다.\n",
        "_, accuracy = mnist_classifier.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "metadata": {
        "id": "Ik5wfrL49EuM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
